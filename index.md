---
layout: default
title: Home
---

<div class="course-header">
  <h1>CIS6200: Deep Learning at Scale</h1>
  <p class="subtitle">Spring 2026 | University of Pennsylvania</p>
  <p class="instructor">Instructor: Lyle Ungar</p>
</div>

## Course Overview

This graduate seminar explores the systems, algorithms, and engineering principles behind training large-scale deep learning models. We will examine how modern AI systems—from GPT-4 to Gemini to open-source alternatives—are designed, trained, and deployed at scale.

The course combines theoretical foundations with practical implementation, covering distributed training systems, optimization techniques, model architectures, and the infrastructure that makes large-scale AI possible.

## Prerequisites

- **Required**: Graduate standing or instructor permission
- **Strongly Recommended**:
  - CIS 5200 (Machine Learning) or equivalent
  - Familiarity with Python and PyTorch
  - Basic understanding of neural networks
  - Comfort with linear algebra and calculus

## Grading

| Component | Weight | Description |
|-----------|--------|-------------|
| Paper Presentations | 25% | Lead discussion of 1-2 assigned papers |
| Class Participation | 15% | Active engagement in discussions and polls |
| Reading Responses | 20% | Weekly written responses to readings |
| Assignments | 15% | 2-3 implementation assignments |
| Final Project | 25% | Research project with paper and presentation |

## Major Course Themes

1. **Foundations of Scale**
   - Why scale matters in deep learning
   - Scaling laws and emergent capabilities
   - Hardware landscape: GPUs, TPUs, and beyond

2. **Distributed Training Systems**
   - Data parallelism and model parallelism
   - Pipeline parallelism and tensor parallelism
   - Communication primitives and collective operations

3. **Optimization at Scale**
   - Large-batch training techniques
   - Learning rate schedules and warmup
   - Gradient compression and mixed precision

4. **Model Architectures**
   - Transformer architecture deep dive
   - Mixture of experts
   - Efficient attention mechanisms

5. **Training Infrastructure**
   - Checkpointing and fault tolerance
   - Memory optimization techniques
   - Profiling and debugging at scale

6. **Post-Training and Deployment**
   - Fine-tuning and RLHF
   - Inference optimization
   - Model compression and quantization

## Course Logistics

- **Time**: Tuesday/Thursday 3:30-5:00 PM
- **Location**: Levine 307
- **Office Hours**: Thursday 5:00-6:00 PM (after class) or by appointment

See the [Schedule](schedule) for weekly topics and readings, and [Resources](resources) for course infrastructure and materials.
