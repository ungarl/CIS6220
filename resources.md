---
layout: default
title: Resources
---

## Course Infrastructure

| Platform | Purpose | Link |
|----------|---------|------|
| Canvas | Grades, announcements, assignment submission | [Penn Canvas](https://canvas.upenn.edu) |
| Google Drive | Lecture slides, shared materials | Link on Canvas |
| Ed Discussion | Questions, discussions, announcements | Link on Canvas |
| PollEverywhere | In-class participation | Link on Canvas |

## Weekly Cadence

Each week follows a consistent rhythm:

- **Before Tuesday**: Complete required readings for the week
- **Tuesday Class**: Lecture and discussion on first topic
- **Before Thursday**: Prepare discussion questions, complete any additional readings
- **Thursday Class**: Paper discussion led by student presenters
- **Friday 11:59 PM**: Reading response due on Canvas

## How to Read Papers

Reading research papers is a skill. Here's an approach that works well:

### First Pass (15-20 minutes)
1. Read the title, abstract, and introduction
2. Look at all figures, tables, and their captions
3. Read the conclusion
4. Skim section headings to understand structure

**Goal**: Understand what problem they're solving and their main contribution

### Second Pass (45-60 minutes)
1. Read the paper more carefully, but skip dense math on first read
2. Make note of terms or concepts you don't understand
3. Pay attention to experimental setup and results
4. Look up 2-3 key references if needed

**Goal**: Understand their approach and be able to summarize it

### Third Pass (optional, for deep understanding)
1. Work through the math and proofs
2. Think about what assumptions they make
3. Consider what's missing or what you'd do differently
4. Try to mentally "re-implement" their approach

**Goal**: Deep understanding, ability to extend or critique

### Questions to Consider
- What problem are they solving? Why does it matter?
- What is their key insight or contribution?
- What are the limitations or assumptions?
- How do their results compare to prior work?
- What experiments would you run that they didn't?
- How does this connect to other papers we've read?

## Additional Resources

### Textbooks and Courses
- [Dive into Deep Learning](https://d2l.ai/) — Interactive deep learning textbook
- [CS336: Language Modeling from Scratch](https://stanford-cs336.github.io/spring2024/) — Stanford course on LLM training
- [Full Stack Deep Learning](https://fullstackdeeplearning.com/) — Production ML course

### Technical References
- [PyTorch Distributed Documentation](https://pytorch.org/docs/stable/distributed.html)
- [NVIDIA Deep Learning Performance Guide](https://docs.nvidia.com/deeplearning/performance/index.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

### Blogs and Articles
- [Lilian Weng's Blog](https://lilianweng.github.io/) — Excellent ML research summaries
- [The Gradient](https://thegradient.pub/) — ML research perspectives
- [Chip Huyen's Blog](https://huyenchip.com/blog/) — ML systems and engineering

### Tools We'll Use
- **PyTorch** — Primary deep learning framework
- **Hugging Face** — Models, datasets, and training utilities

## Paper Reading List by Topic

### Scaling Laws
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- [Scaling Laws for Autoregressive Generative Modeling](https://arxiv.org/abs/2010.14701)

### Distributed Training
- [Megatron-LM](https://arxiv.org/abs/1909.08053)
- [GPipe](https://arxiv.org/abs/1811.06965)
- [ZeRO](https://arxiv.org/abs/1910.02054)
- [PyTorch FSDP](https://arxiv.org/abs/2304.11277)

### Efficient Architectures
- [FlashAttention](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)
- [Mixture of Experts](https://arxiv.org/abs/2101.03961)

### Large Language Models
- [GPT-3](https://arxiv.org/abs/2005.14165)
- [LLaMA](https://arxiv.org/abs/2302.13971)
- [LLaMA 2](https://arxiv.org/abs/2307.09288)
- [Mistral 7B](https://arxiv.org/abs/2310.06825)

### Post-Training
- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Constitutional AI](https://arxiv.org/abs/2212.08073)
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

### Inference
- [vLLM](https://arxiv.org/abs/2309.06180)
- [Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [AWQ Quantization](https://arxiv.org/abs/2306.00978)

## Getting Help

- **Ed Discussion**: Best for questions that might help others
- **Office Hours**:  TBD
- **Email**: Only for things not related to the course; use Ed for technical questions; do private posts on Ed for personal problems, grading issues, etc.
